{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77ddc1f",
   "metadata": {},
   "source": [
    "Reminder:  Please see VISUAL_OVERVIEW.md and DATA_INVENTORY.md for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84826186",
   "metadata": {},
   "source": [
    "Step 1:  Get the Data File from the /INPUTS directory.  We are looking for .zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7356d",
   "metadata": {},
   "source": [
    "## Setup: Create Virtual Environment (Local Dev Only)\n",
    "Run this cell to create and activate a Python virtual environment for local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416e0192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual environment created at ./venv\n",
      "Virtual environment setup complete!\n",
      "To activate on Windows: .\\./venv\\Scripts\\activate\n",
      "To activate on macOS/Linux: source ./venv/bin/activate\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Create virtual environment\n",
    "venv_dir = './venv'\n",
    "subprocess.run([sys.executable, '-m', 'venv', venv_dir], check=True)\n",
    "print(f'Virtual environment created at {venv_dir}')\n",
    "\n",
    "# Determine pip executable path\n",
    "if os.name == 'nt':  # Windows\n",
    "    pip_executable = os.path.join(venv_dir, 'Scripts', 'pip')\n",
    "else:  # macOS/Linux\n",
    "    pip_executable = os.path.join(venv_dir, 'bin', 'pip')\n",
    "\n",
    "# Install required packages\n",
    "packages = ['pyyaml', 'pydantic', 'sqlalchemy', 'pyodbc', 'pandas', 'numpy']\n",
    "subprocess.run([pip_executable, 'install'] + packages, check=True)\n",
    "\n",
    "print('Virtual environment setup complete!')\n",
    "print(f'To activate on Windows: .\\\\{venv_dir}\\\\Scripts\\\\activate')\n",
    "print(f'To activate on macOS/Linux: source {venv_dir}/bin/activate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813b0160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found INPUTS directory at: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\INPUTS\n",
      "\n",
      "Zip files found:\n",
      "  - GelSightAnalysis.zip\n",
      "\n",
      "Extracting GelSightAnalysis.zip...\n",
      "âœ“ Extracted to: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\data\\GelSightAnalysis\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Locate the .zip input file from /INPUTS directory\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to /INPUTS directory (one level up from Notebooks)\n",
    "notebook_dir = Path.cwd()\n",
    "inputs_dir = Path('..') / 'INPUTS'  # One level up to workspace root, then INPUTS\n",
    "inputs_dir = inputs_dir.resolve()\n",
    "\n",
    "# Create working directory for extracted data\n",
    "work_dir = Path('./data').resolve()\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Find .zip files in INPUTS\n",
    "if inputs_dir.exists():\n",
    "    zip_files = list(inputs_dir.glob('*.zip'))\n",
    "    print(f'âœ“ Found INPUTS directory at: {inputs_dir}')\n",
    "    print(f'\\nZip files found:')\n",
    "    for zip_file in zip_files:\n",
    "        print(f'  - {zip_file.name}')\n",
    "    \n",
    "    # Extract the first (or only) zip file\n",
    "    if zip_files:\n",
    "        zip_path = zip_files[0]\n",
    "        extract_to = work_dir / zip_path.stem\n",
    "        extract_to.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f'\\nExtracting {zip_path.name}...')\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f'âœ“ Extracted to: {extract_to}')\n",
    "    else:\n",
    "        print('âœ— No .zip files found')\n",
    "else:\n",
    "    print(f'âœ— INPUTS directory not found at: {inputs_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90185d",
   "metadata": {},
   "source": [
    "## Step 2: Parse YAML Files and Convert to JSON\n",
    "Extract scan metadata, analysis results, and calibration data from the zip contents and organize into bronze layer structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb8dba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Processing GelSightAnalysis folder: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\data\\GelSightAnalysis\\GelSightAnalysis\n",
      "\n",
      "\n",
      "Processing: DefectDetection\n",
      "  âœ“ scan.yaml -> scan_metadata.json\n",
      "  âœ“ scancontext.yaml -> analysis_results.json\n",
      "  âœ“ Calib-4E07-XNJU_20250917_1255.yaml -> JSON\n",
      "  âœ“ Copied 1 .tmd file(s)\n",
      "  âœ“ Copied 15 PNG image(s)\n",
      "  âœ“ Preserved raw scan.yaml\n",
      "  âœ“ Preserved raw scancontext.yaml\n",
      "\n",
      "Processing: HoleDiameter\n",
      "  âœ“ scan.yaml -> scan_metadata.json\n",
      "  âœ“ scancontext.yaml -> analysis_results.json\n",
      "  âœ“ Calib-4E07-XNJU_20250917_1255.yaml -> JSON\n",
      "  âœ“ Copied 1 .tmd file(s)\n",
      "  âœ“ Copied 15 PNG image(s)\n",
      "  âœ“ Preserved raw scan.yaml\n",
      "  âœ“ Preserved raw scancontext.yaml\n",
      "\n",
      "Processing: Offset\n",
      "  âœ“ scan.yaml -> scan_metadata.json\n",
      "  âœ“ scancontext.yaml -> analysis_results.json\n",
      "  âœ“ Calib-4E07-XNJU_20250917_1255.yaml -> JSON\n",
      "  âœ“ Copied 1 .tmd file(s)\n",
      "  âœ“ Copied 15 PNG image(s)\n",
      "  âœ“ Preserved raw scan.yaml\n",
      "  âœ“ Preserved raw scancontext.yaml\n",
      "\n",
      "Processing: PitDetection\n",
      "  âœ“ scan.yaml -> scan_metadata.json\n",
      "  âœ“ scancontext.yaml -> analysis_results.json\n",
      "  âœ“ Calib-4E07-XNJU_20250917_1255.yaml -> JSON\n",
      "  âœ“ Copied 1 .tmd file(s)\n",
      "  âœ“ Copied 15 PNG image(s)\n",
      "  âœ“ Preserved raw scan.yaml\n",
      "  âœ“ Preserved raw scancontext.yaml\n",
      "\n",
      "Processing: Reports\n",
      "  âœ— scan.yaml not found at C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\data\\GelSightAnalysis\\GelSightAnalysis\\Reports\\scan.yaml\n",
      "  âœ— analysis/scancontext.yaml not found\n",
      "\n",
      "Processing: SurfaceRoughness\n",
      "  âœ“ scan.yaml -> scan_metadata.json\n",
      "  âœ“ scancontext.yaml -> analysis_results.json\n",
      "  âœ“ Calib-436U-Y58K_20250912_0928.yaml -> JSON\n",
      "  âœ“ Copied 1 .tmd file(s)\n",
      "  âœ“ Copied 8 PNG image(s)\n",
      "  âœ“ Preserved raw scan.yaml\n",
      "  âœ“ Preserved raw scancontext.yaml\n",
      "\n",
      "\n",
      "âœ“ Data extraction complete!\n",
      "\n",
      "Output structure:\n",
      "  Bronze scans: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\OUTPUTS\\customer01\\bronze\\scans\n",
      "  Bronze calibrations: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\OUTPUTS\\customer01\\bronze\\calibrations\n",
      "\n",
      "Each scan folder contains:\n",
      "  - scan_metadata.json\n",
      "  - analysis_results.json\n",
      "  - images/ (PNG files)\n",
      "  - heightmaps/ (.tmd files)\n",
      "  - raw_yaml/ (original YAML files for audit)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "import shutil\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "# Helper function to convert non-serializable types for JSON\n",
    "def convert_nan(obj):\n",
    "    \"\"\"Recursively convert NaN, Inf, datetime, and other non-JSON-serializable values\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_nan(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_nan(item) for item in obj]\n",
    "    elif isinstance(obj, (datetime, date, time)):\n",
    "        return obj.isoformat()  # Convert datetime to ISO format string\n",
    "    elif isinstance(obj, float):\n",
    "        if math.isnan(obj) or math.isinf(obj):\n",
    "            return None\n",
    "    return obj\n",
    "\n",
    "# Define paths\n",
    "work_dir = Path('./data').resolve()\n",
    "outputs_dir = Path('./OUTPUTS').resolve()\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "bronze_scans = outputs_dir / 'customer01' / 'bronze' / 'scans'\n",
    "bronze_calibrations = outputs_dir / 'customer01' / 'bronze' / 'calibrations'\n",
    "bronze_scans.mkdir(parents=True, exist_ok=True)\n",
    "bronze_calibrations.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find the extracted GelSightAnalysis folder\n",
    "# Handle nested GelSightAnalysis/GelSightAnalysis structures\n",
    "gelsight_dir = work_dir / 'GelSightAnalysis'\n",
    "\n",
    "# Check if there's a nested GelSightAnalysis folder\n",
    "if gelsight_dir.exists():\n",
    "    nested_gel = gelsight_dir / 'GelSightAnalysis'\n",
    "    if nested_gel.exists():\n",
    "        gelsight_dir = nested_gel\n",
    "\n",
    "# If still not found, try glob pattern\n",
    "if not gelsight_dir.exists():\n",
    "    extracted_folders = list(work_dir.glob('**/GelSightAnalysis'))\n",
    "    if extracted_folders:\n",
    "        # Use the deepest nested one\n",
    "        gelsight_dir = extracted_folders[-1]\n",
    "\n",
    "if gelsight_dir.exists():\n",
    "    print(f'âœ“ Processing GelSightAnalysis folder: {gelsight_dir}\\n')\n",
    "    \n",
    "    # Process each analysis type folder (DefectDetection, HoleDiameter, etc.)\n",
    "    analysis_folders = [d for d in gelsight_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not analysis_folders:\n",
    "        print(f'âœ— No analysis folders found in {gelsight_dir}')\n",
    "        print(f'Contents: {list(gelsight_dir.iterdir())}')\n",
    "    \n",
    "    for analysis_folder in sorted(analysis_folders):\n",
    "        analysis_type = analysis_folder.name\n",
    "        print(f'\\nProcessing: {analysis_type}')\n",
    "        \n",
    "        # Create folder for this scan\n",
    "        scan_id = analysis_type\n",
    "        scan_folder = bronze_scans / scan_id\n",
    "        scan_folder.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Extract scan.yaml\n",
    "        scan_yaml = analysis_folder / 'scan.yaml'\n",
    "        if scan_yaml.exists():\n",
    "            with open(scan_yaml, 'r') as f:\n",
    "                scan_data = yaml.safe_load(f)\n",
    "            # Save as JSON (convert non-serializable types)\n",
    "            scan_json = scan_folder / 'scan_metadata.json'\n",
    "            with open(scan_json, 'w') as f:\n",
    "                json.dump(convert_nan(scan_data), f, indent=2)\n",
    "            print(f'  âœ“ scan.yaml -> scan_metadata.json')\n",
    "        else:\n",
    "            print(f'  âœ— scan.yaml not found at {scan_yaml}')\n",
    "        \n",
    "        # Extract analysis/scancontext.yaml\n",
    "        analysis_yaml = analysis_folder / 'analysis' / 'scancontext.yaml'\n",
    "        if analysis_yaml.exists():\n",
    "            with open(analysis_yaml, 'r') as f:\n",
    "                analysis_data = yaml.safe_load(f)\n",
    "            # Save as JSON (convert non-serializable types)\n",
    "            analysis_json = scan_folder / 'analysis_results.json'\n",
    "            with open(analysis_json, 'w') as f:\n",
    "                json.dump(convert_nan(analysis_data), f, indent=2)\n",
    "            print(f'  âœ“ scancontext.yaml -> analysis_results.json')\n",
    "        else:\n",
    "            print(f'  âœ— analysis/scancontext.yaml not found')\n",
    "        \n",
    "        # Extract calibration file (Calib-*.yaml)\n",
    "        calib_files = list(analysis_folder.glob('Calib-*.yaml'))\n",
    "        for calib_file in calib_files:\n",
    "            with open(calib_file, 'r') as f:\n",
    "                calib_data = yaml.safe_load(f)\n",
    "            # Save as JSON in calibrations folder (convert non-serializable types)\n",
    "            calib_id = calib_file.stem\n",
    "            calib_folder = bronze_calibrations / calib_id\n",
    "            calib_folder.mkdir(exist_ok=True)\n",
    "            calib_json = calib_folder / 'calib_metadata.json'\n",
    "            with open(calib_json, 'w') as f:\n",
    "                json.dump(convert_nan(calib_data), f, indent=2)\n",
    "            print(f'  âœ“ {calib_file.name} -> JSON')\n",
    "        \n",
    "        # Copy .tmd files if they exist\n",
    "        tmd_files = list(analysis_folder.glob('*.tmd'))\n",
    "        heightmaps_dir = scan_folder / 'heightmaps'\n",
    "        if tmd_files:\n",
    "            heightmaps_dir.mkdir(exist_ok=True)\n",
    "            for tmd_file in tmd_files:\n",
    "                shutil.copy(tmd_file, heightmaps_dir / tmd_file.name)\n",
    "            print(f'  âœ“ Copied {len(tmd_files)} .tmd file(s)')\n",
    "        \n",
    "        # Copy PNG images if they exist\n",
    "        png_files = list(analysis_folder.glob('*.png'))\n",
    "        if png_files:\n",
    "            images_dir = scan_folder / 'images'\n",
    "            images_dir.mkdir(exist_ok=True)\n",
    "            for png_file in png_files:\n",
    "                shutil.copy(png_file, images_dir / png_file.name)\n",
    "            print(f'  âœ“ Copied {len(png_files)} PNG image(s)')\n",
    "        \n",
    "        # Copy raw YAML files for audit trail\n",
    "        raw_yaml_dir = scan_folder / 'raw_yaml'\n",
    "        raw_yaml_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Copy scan.yaml\n",
    "        if scan_yaml.exists():\n",
    "            shutil.copy(scan_yaml, raw_yaml_dir / 'scan.yaml')\n",
    "            print(f'  âœ“ Preserved raw scan.yaml')\n",
    "        \n",
    "        # Copy scancontext.yaml\n",
    "        if analysis_yaml.exists():\n",
    "            shutil.copy(analysis_yaml, raw_yaml_dir / 'scancontext.yaml')\n",
    "            print(f'  âœ“ Preserved raw scancontext.yaml')\n",
    "        \n",
    "        # Copy calibration files\n",
    "        for calib_file in calib_files:\n",
    "            shutil.copy(calib_file, raw_yaml_dir / calib_file.name)\n",
    "    \n",
    "    print(f'\\n\\nâœ“ Data extraction complete!')\n",
    "    print(f'\\nOutput structure:')\n",
    "    print(f'  Bronze scans: {bronze_scans}')\n",
    "    print(f'  Bronze calibrations: {bronze_calibrations}')\n",
    "    print(f'\\nEach scan folder contains:')\n",
    "    print(f'  - scan_metadata.json')\n",
    "    print(f'  - analysis_results.json')\n",
    "    print(f'  - images/ (PNG files)')\n",
    "    print(f'  - heightmaps/ (.tmd files)')\n",
    "    print(f'  - raw_yaml/ (original YAML files for audit)')\n",
    "else:\n",
    "    print(f'âœ— GelSightAnalysis folder not found')\n",
    "    print(f'Searched in: {work_dir}')\n",
    "    print(f'Contents: {list(work_dir.iterdir()) if work_dir.exists() else \"work_dir does not exist\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb7b01",
   "metadata": {},
   "source": [
    "## Step 3: Load Bronze Data into DataFrames and Transform to Silver Layer\n",
    "Combine scan metadata, analysis results, and calibration data into unified DataFrames and save to silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d82e3ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loading bronze layer data from: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\OUTPUTS\\customer01\\bronze\\scans\n",
      "\n",
      "  âœ“ Loaded scan_metadata.json from DefectDetection\n",
      "  âœ“ Loaded analysis_results.json from DefectDetection\n",
      "  âœ“ Loaded scan_metadata.json from HoleDiameter\n",
      "  âœ“ Loaded analysis_results.json from HoleDiameter\n",
      "  âœ“ Loaded scan_metadata.json from Offset\n",
      "  âœ“ Loaded analysis_results.json from Offset\n",
      "  âœ“ Loaded scan_metadata.json from PitDetection\n",
      "  âœ“ Loaded analysis_results.json from PitDetection\n",
      "  âœ“ Loaded scan_metadata.json from SurfaceRoughness\n",
      "  âœ“ Loaded analysis_results.json from SurfaceRoughness\n",
      "\n",
      "âœ“ Created scans DataFrame: 5 rows, 38 columns\n",
      "  Columns: ['version', 'guid', 'createdon', 'mmperpixel', 'sdkversion']...\n",
      "âœ“ Created analysis DataFrame: 5 rows, 4 columns\n",
      "  Columns: ['version', 'shapes', 'routines', 'scan_id']...\n",
      "\n",
      "âœ“ Merged scans + analysis: 5 rows\n",
      "  âœ“ Saved to: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\OUTPUTS\\customer01\\silver\\scans\\scans_merged.csv\n",
      "  âœ“ Saved summary to: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\OUTPUTS\\customer01\\silver\\scans\\data_summary.json\n",
      "\n",
      "âœ“ Loading calibration data from: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\OUTPUTS\\customer01\\bronze\\calibrations\n",
      "  âœ“ Loaded Calib-436U-Y58K_20250912_0928\n",
      "  âœ“ Loaded Calib-4E07-XNJU_20250917_1255\n",
      "\n",
      "âœ“ Created calibrations DataFrame: 2 rows, 58 columns\n",
      "  âœ“ Saved to: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\OUTPUTS\\customer01\\silver\\calibrations\\calibrations.csv\n",
      "\n",
      "âœ“ Silver layer transformation complete!\n",
      "\n",
      "Output location: C:\\Projects\\Clients\\GelSight\\Gelsight Application Folder\\Notebooks\\OUTPUTS\\customer01\\silver\n",
      "  scans_merged.csv - Merged scan + analysis data\n",
      "  calibrations.csv - Calibration data\n",
      "  data_summary.json - Metadata about the merged data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Define paths\n",
    "outputs_dir = Path('./OUTPUTS').resolve()\n",
    "bronze_scans = outputs_dir / 'customer01' / 'bronze' / 'scans'\n",
    "bronze_calibrations = outputs_dir / 'customer01' / 'bronze' / 'calibrations'\n",
    "\n",
    "silver_scans = outputs_dir / 'customer01' / 'silver' / 'scans'\n",
    "silver_calibrations = outputs_dir / 'customer01' / 'silver' / 'calibrations'\n",
    "silver_scans.mkdir(parents=True, exist_ok=True)\n",
    "silver_calibrations.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'âœ“ Loading bronze layer data from: {bronze_scans}\\n')\n",
    "\n",
    "# Load all scan metadata and analysis results into DataFrames\n",
    "scan_records = []\n",
    "analysis_records = []\n",
    "\n",
    "if bronze_scans.exists():\n",
    "    scan_folders = [d for d in bronze_scans.iterdir() if d.is_dir()]\n",
    "    \n",
    "    for scan_folder in sorted(scan_folders):\n",
    "        scan_id = scan_folder.name\n",
    "        \n",
    "        # Load scan metadata\n",
    "        scan_json = scan_folder / 'scan_metadata.json'\n",
    "        if scan_json.exists():\n",
    "            try:\n",
    "                with open(scan_json, 'r') as f:\n",
    "                    scan_data = json.load(f)\n",
    "                # Add scan_id for later joining\n",
    "                scan_data['scan_id'] = scan_id\n",
    "                scan_records.append(scan_data)\n",
    "                print(f'  âœ“ Loaded scan_metadata.json from {scan_id}')\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f'  âœ— Invalid JSON in {scan_id}/scan_metadata.json: {e}')\n",
    "        \n",
    "        # Load analysis results\n",
    "        analysis_json = scan_folder / 'analysis_results.json'\n",
    "        if analysis_json.exists():\n",
    "            try:\n",
    "                with open(analysis_json, 'r') as f:\n",
    "                    analysis_data = json.load(f)\n",
    "                # Add scan_id for joining\n",
    "                analysis_data['scan_id'] = scan_id\n",
    "                analysis_records.append(analysis_data)\n",
    "                print(f'  âœ“ Loaded analysis_results.json from {scan_id}')\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f'  âœ— Invalid JSON in {scan_id}/analysis_results.json: {e}')\n",
    "    \n",
    "    # Create DataFrames with error handling for nested structures\n",
    "    try:\n",
    "        # Flatten and create scans DataFrame\n",
    "        scans_df = pd.json_normalize(scan_records) if scan_records else pd.DataFrame()\n",
    "        print(f'\\nâœ“ Created scans DataFrame: {len(scans_df)} rows, {len(scans_df.columns)} columns')\n",
    "        print(f'  Columns: {list(scans_df.columns)[:5]}...')  # Show first 5 columns\n",
    "        \n",
    "        # Flatten and create analysis DataFrame\n",
    "        analysis_df = pd.json_normalize(analysis_records) if analysis_records else pd.DataFrame()\n",
    "        print(f'âœ“ Created analysis DataFrame: {len(analysis_df)} rows, {len(analysis_df.columns)} columns')\n",
    "        print(f'  Columns: {list(analysis_df.columns)[:5]}...')\n",
    "        \n",
    "        # Merge scans and analysis on scan_id if both exist\n",
    "        if not scans_df.empty and not analysis_df.empty:\n",
    "            merged_df = scans_df.merge(analysis_df, on='scan_id', how='left', suffixes=('_scan', '_analysis'))\n",
    "            print(f'\\nâœ“ Merged scans + analysis: {len(merged_df)} rows')\n",
    "        else:\n",
    "            merged_df = scans_df if not scans_df.empty else analysis_df\n",
    "        \n",
    "        # Save silver layer data as CSV\n",
    "        if not merged_df.empty:\n",
    "            # Save as CSV (readable, portable)\n",
    "            silver_scans_csv = silver_scans / 'scans_merged.csv'\n",
    "            merged_df.to_csv(silver_scans_csv, index=False)\n",
    "            print(f'  âœ“ Saved to: {silver_scans_csv}')\n",
    "            \n",
    "            # Save summary statistics\n",
    "            summary = {\n",
    "                'total_records': len(merged_df),\n",
    "                'total_columns': len(merged_df.columns),\n",
    "                'columns': list(merged_df.columns),\n",
    "                'dtypes': merged_df.dtypes.astype(str).to_dict(),\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "            summary_json = silver_scans / 'data_summary.json'\n",
    "            with open(summary_json, 'w') as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            print(f'  âœ“ Saved summary to: {summary_json}')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'\\nâœ— Error during DataFrame creation: {e}')\n",
    "        print(f'  This may indicate nested/complex structures. Raw data still available in bronze layer.')\n",
    "\n",
    "# Load and process calibrations\n",
    "calibration_records = []\n",
    "if bronze_calibrations.exists():\n",
    "    calib_folders = [d for d in bronze_calibrations.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f'\\nâœ“ Loading calibration data from: {bronze_calibrations}')\n",
    "    for calib_folder in sorted(calib_folders):\n",
    "        calib_id = calib_folder.name\n",
    "        calib_json = calib_folder / 'calib_metadata.json'\n",
    "        if calib_json.exists():\n",
    "            with open(calib_json, 'r') as f:\n",
    "                calib_data = json.load(f)\n",
    "            calib_data['calib_id'] = calib_id\n",
    "            calibration_records.append(calib_data)\n",
    "            print(f'  âœ“ Loaded {calib_id}')\n",
    "    \n",
    "    if calibration_records:\n",
    "        calib_df = pd.json_normalize(calibration_records)\n",
    "        print(f'\\nâœ“ Created calibrations DataFrame: {len(calib_df)} rows, {len(calib_df.columns)} columns')\n",
    "        \n",
    "        # Save silver layer calibrations as CSV\n",
    "        silver_calib_csv = silver_calibrations / 'calibrations.csv'\n",
    "        calib_df.to_csv(silver_calib_csv, index=False)\n",
    "        print(f'  âœ“ Saved to: {silver_calib_csv}')\n",
    "\n",
    "print(f'\\nâœ“ Silver layer transformation complete!')\n",
    "print(f'\\nOutput location: {outputs_dir / \"customer01\" / \"silver\"}')\n",
    "print(f'  scans_merged.csv - Merged scan + analysis data')\n",
    "print(f'  calibrations.csv - Calibration data')\n",
    "print(f'  data_summary.json - Metadata about the merged data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ddad91",
   "metadata": {},
   "source": [
    "## Step 4: Create Databricks Tables from Silver Layer\n",
    "**Create External Tables in Unity Catalog**\n",
    "\n",
    "### What We've Accomplished\n",
    "We have successfully extracted and transformed GelSight analysis data into two storage layers:\n",
    "\n",
    "- **Bronze Layer**: Raw data organized by analysis type (stored as JSON backup/audit trail)\n",
    "  - `OUTPUTS/customer01/bronze/scans/` - Individual scan folders with JSON metadata\n",
    "  - `OUTPUTS/customer01/bronze/calibrations/` - Calibration data in JSON format\n",
    "  - *Keep these as-is for audit trail; no tables needed*\n",
    "  \n",
    "- **Silver Layer**: Merged, flattened DataFrames ready for analytics (stored as CSV)\n",
    "  - `OUTPUTS/customer01/silver/scans/scans_merged.csv` - 5 scans Ã— 42 columns (merged scan metadata + analysis results)\n",
    "  - `OUTPUTS/customer01/silver/calibrations/calibrations.csv` - 2 calibrations\n",
    "\n",
    "### Create Silver Tables in Databricks\n",
    "We only need to create tables for the **silver layer CSV files** since they're already flattened and ready to query.\n",
    "\n",
    "**Steps:**\n",
    "1. **Upload** `OUTPUTS/customer01/silver/` folder to Azure storage (`abfss://customer01/silver`)\n",
    "2. **Create External Tables** in Databricks Unity Catalog pointing to the CSV files:\n",
    "   - `customer01_data.silver.scans` â†’ Points to `OUTPUTS/customer01/silver/scans/scans_merged.csv`\n",
    "   - `customer01_data.silver.calibrations` â†’ Points to `OUTPUTS/customer01/silver/calibrations/calibrations.csv`\n",
    "3. **Query** the tables for analytics\n",
    "4. **Optionally create Gold Layer** - aggregate/clean silver data into high-level analytical tables\n",
    "\n",
    "### Why Only Silver Tables?\n",
    "- âœ… Silver CSVs are already flattened and normalized (no nested structures)\n",
    "- âœ… Easy to query with standard SQL\n",
    "- âœ… Bronze JSON files stay in storage as backup/audit trail (read-only reference)\n",
    "- âœ… Cleaner, simpler approach for analytics\n",
    "\n",
    "### Data Structure\n",
    "| Layer | Format | Location | Purpose |\n",
    "|-------|--------|----------|---------|\n",
    "| Bronze | JSON | `bronze/scans/`, `bronze/calibrations/` | Raw data, audit trail, backup |\n",
    "| Silver | CSV | `silver/scans/scans_merged.csv`, `silver/calibrations/calibrations.csv` | Analytics-ready, flattened data |\n",
    "| Gold | (optional) | TBD | Aggregated, business-logic tables |\n",
    "\n",
    "### Credentials & Locations\n",
    "- **Databricks Workspace**: USCU-PROD-DATA-PROCESSING-01\n",
    "- **Cluster**: CLU-SCAN-1\n",
    "- **Unity Catalog**: `customer01_data`\n",
    "- **Schemas**: `silver`, `gold` (create as needed)\n",
    "- **External Location**: `uscu_silver` (credentialed with `uscu-storage-cred` managed identity)\n",
    "- **Storage Account**: gelsightprodstnd01\n",
    "- **Container**: customer01\n",
    "\n",
    "### SQL Commands to Execute in Databricks\n",
    "```sql\n",
    "-- Create external table for silver scans\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS customer01_data.silver.scans\n",
    "  USING CSV\n",
    "  LOCATION 'abfss://customer01/silver@gelsightprodstnd01.dfs.core.windows.net/scans/'\n",
    "  WITH (header=true, inferSchema=true)\n",
    "\n",
    "-- Create external table for silver calibrations\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS customer01_data.silver.calibrations\n",
    "  USING CSV\n",
    "  LOCATION 'abfss://customer01/silver@gelsightprodstnd01.dfs.core.windows.net/calibrations/'\n",
    "  WITH (header=true, inferSchema=true)\n",
    "\n",
    "-- Validate the data\n",
    "SELECT COUNT(*) as scan_count FROM customer01_data.silver.scans\n",
    "SELECT COUNT(*) as calib_count FROM customer01_data.silver.calibrations\n",
    "```\n",
    "\n",
    "**Bronze JSON files remain in storage** at `abfss://customer01/bronze/` for backup/audit purposes but do not require Unity Catalog tables unless you specifically need to query the raw nested JSON structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8927de6",
   "metadata": {},
   "source": [
    "## Step 5: Create Gold Layer - Analytics & Reporting Tables\n",
    "\n",
    "**Transform Silver Tables into Business-Ready Analytics**\n",
    "\n",
    "### Gold Layer Purpose\n",
    "Build analytical tables optimized for:\n",
    "- âœ… Master dimension of all scans (scan_registry)\n",
    "- âœ… Scan-to-calibration traceability (real relationships from source data)\n",
    "- âœ… Analysis-type breakdown for comparison\n",
    "- âœ… Calibration usage metrics and statistics\n",
    "- âœ… Dashboards and reporting\n",
    "\n",
    "### Gold Layer Tables to Create\n",
    "1. **`customer01_data.gold.scan_registry`** - Master dimension: all scan metadata with extracted calibration ID\n",
    "2. **`customer01_data.gold.analysis_by_type`** - Results grouped by analysis type (DefectDetection, PitDetection, etc.)\n",
    "3. **`customer01_data.gold.calibration_usage`** - Calibration usage stats: which calibrations are used, by which scans, date ranges\n",
    "\n",
    "### Key Finding: Real Calibration Relationships! \n",
    "âœ… Each scan has a `calib` column with the calibration file path\n",
    "- 4 scans â†’ Calib-4E07-XNJU_20250917_1255\n",
    "- 1 scan â†’ Calib-436U-Y58K_20250912_0928\n",
    "\n",
    "### Next: Execute SQL Commands Below\n",
    "Run the SQL cells below to create the gold layer tables in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d065dd",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Gold Layer: Scan Registry (Master Dimension Table)\n",
    "-- All scans with metadata and extracted calibration ID from file path\n",
    "CREATE OR REPLACE TABLE customer01_data.gold.scan_registry AS\n",
    "SELECT \n",
    "    scan_id,\n",
    "    guid,\n",
    "    createdon,\n",
    "    mmPerPixel,\n",
    "    sdkversion,\n",
    "    -- Extract calibration ID from calib file path (e.g., 'Calib-4E07-XNJU_20250917_1255')\n",
    "    REGEXP_EXTRACT(calib, 'Calib-[^/\\\\]+') as calib_id,\n",
    "    calib as calib_filepath\n",
    "FROM customer01_data.silver.scans\n",
    "ORDER BY createdon DESC;\n",
    "\n",
    "-- Validate scan registry\n",
    "SELECT * FROM customer01_data.gold.scan_registry;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c56231",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Gold Layer: Analysis By Type\n",
    "-- Aggregates results by analysis type (DefectDetection, PitDetection, etc.)\n",
    "CREATE OR REPLACE TABLE customer01_data.gold.analysis_by_type AS\n",
    "SELECT \n",
    "    scan_id,\n",
    "    createdon,\n",
    "    -- Extract analysis type from scan_id (first part before any underscore/suffix)\n",
    "    CASE \n",
    "        WHEN scan_id LIKE 'DefectDetection%' THEN 'Defect Detection'\n",
    "        WHEN scan_id LIKE 'PitDetection%' THEN 'Pit Detection'\n",
    "        WHEN scan_id LIKE 'HoleDiameter%' THEN 'Hole Diameter'\n",
    "        WHEN scan_id LIKE 'Offset%' THEN 'Offset'\n",
    "        WHEN scan_id LIKE 'SurfaceRoughness%' THEN 'Surface Roughness'\n",
    "        ELSE scan_id\n",
    "    END as analysis_type,\n",
    "    mmPerPixel,\n",
    "    COUNT(*) as record_count\n",
    "FROM customer01_data.silver.scans\n",
    "GROUP BY scan_id, createdon, analysis_type, mmPerPixel\n",
    "ORDER BY createdon DESC, analysis_type;\n",
    "\n",
    "-- Validate analysis by type\n",
    "SELECT * FROM customer01_data.gold.analysis_by_type;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7b81d",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Gold Layer: Calibration Usage\n",
    "-- Shows which calibrations are used, by which scans, and date coverage\n",
    "CREATE OR REPLACE TABLE customer01_data.gold.calibration_usage AS\n",
    "SELECT \n",
    "    sr.calib_id,\n",
    "    COUNT(DISTINCT sr.scan_id) as scan_count,\n",
    "    COLLECT_LIST(sr.scan_id) as scan_ids,\n",
    "    MIN(sr.createdon) as earliest_scan_date,\n",
    "    MAX(sr.createdon) as latest_scan_date,\n",
    "    -- Join with calibrations table for calibration metadata\n",
    "    MAX(c.calib_id) as calibration_record_id\n",
    "FROM customer01_data.gold.scan_registry sr\n",
    "LEFT JOIN customer01_data.silver.calibrations c\n",
    "  ON sr.calib_id = c.calib_id\n",
    "WHERE sr.calib_id IS NOT NULL\n",
    "GROUP BY sr.calib_id\n",
    "ORDER BY latest_scan_date DESC;\n",
    "\n",
    "-- Validate calibration usage\n",
    "SELECT * FROM customer01_data.gold.calibration_usage;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19331746",
   "metadata": {},
   "source": [
    "### Gold Layer Complete! ðŸŽ¯\n",
    "\n",
    "You now have 3 analytics tables ready for dashboards, reports, and decision-making:\n",
    "\n",
    "| Table | Purpose | Use Case |\n",
    "|-------|---------|----------|\n",
    "| `gold.scan_registry` | Master dimension with calibration IDs | Scan lookup, audit trails, traceability |\n",
    "| `gold.analysis_by_type` | Results grouped by analysis type | Defect vs pit vs roughness comparison |\n",
    "| `gold.calibration_usage` | Calibration statistics & usage | Track which calibrations are used and for how long |\n",
    "\n",
    "### Real Data Insights\n",
    "âœ… **Calibration Distribution**:\n",
    "- Calib-4E07-XNJU_20250917_1255: 4 scans (9/17-9/29)\n",
    "- Calib-436U-Y58K_20250912_0928: 1 scan (9/12)\n",
    "\n",
    "### Next Steps\n",
    "- **Build Dashboards** using Databricks SQL dashboards or connect to BI tools (Power BI, Tableau)\n",
    "- **Create Alerts** on quality metrics or calibration age\n",
    "- **Add More Gold Tables** as business needs evolve:\n",
    "  - Defect trends by calibration\n",
    "  - Scan quality scores with calibration traceability\n",
    "  - Surface roughness statistics by analysis type\n",
    "- **Document Lineage** - Track data from raw GelSight scans â†’ Bronze â†’ Silver â†’ Gold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2908e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
